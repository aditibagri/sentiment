import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


# Load pre-trained word embeddings (GloVe)
embeddings_index = {}
with open('glove.6B.100d.txt', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Define function to create document vectors from embeddings
def document_vector(doc):
    # Remove stop words and tokenize document text
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(doc.lower())
    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]
    
    # Calculate weighted average of embeddings for each token
    word_vectors = []
    for token in filtered_tokens:
        if token in embeddings_index:
            word_vectors.append(embeddings_index[token])
    if not word_vectors:
        return np.zeros(100)
    doc_vector = np.average(word_vectors, axis=0, weights=tfidf.transform([doc]).toarray()[0])
    return doc_vector

# Load and preprocess training data
train_data = pd.read_csv('train_data.csv')
train_labels = train_data['label']
train_data = train_data['text']

# Create TF-IDF matrix of training data
tfidf = TfidfVectorizer(tokenizer=word_tokenize, stop_words='english')
train_tfidf = tfidf.fit_transform(train_data)

# Create document vectors from embeddings for training data
train_vectors = np.array([document_vector(doc) for doc in train_data])

# Train Naive Bayes classifier on document vectors
nb = MultinomialNB()
nb.fit(train_vectors, train_labels)

# Load and preprocess test data
test_data = pd.read_csv('test_data.csv')
test_labels = test_data['label']
test_data = test_data['text']

# Create document vectors from embeddings for test data
test_vectors = np.array([document_vector(doc) for doc in test_data])

# Make predictions on test data
pred_labels = nb.predict(test_vectors)

# Evaluate accuracy of predictions
accuracy = accuracy_score(test_labels, pred_labels)
print("Accuracy:", accuracy)
